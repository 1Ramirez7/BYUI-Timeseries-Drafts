{
  "hash": "b5e7e4267e43abb4836b0de9f6008015",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Autoregressive (AR) Models\"\nsubtitle: \"Chapter 4: Lesson 3\"\nformat: html\neditor: source\nsidebar: false\n---\n\n\n\n```{=html}\n<script type=\"text/javascript\">\n function showhide(id) {\n    var e = document.getElementById(id);\n    // Ensure the element is always shown\n    e.style.display = 'block';\n }\n \n function openTab(evt, tabName) {\n    var i, tablinks;\n    // Skip hiding tab content\n    var tabcontent = document.getElementsByClassName(\"tabcontent\");\n    for (i = 0; i < tabcontent.length; i++) {\n        tabcontent[i].style.display = \"block\"; // Keep everything visible\n    }\n    tablinks = document.getElementsByClassName(\"tablinks\");\n    for (i = 0; i < tablinks.length; i++) {\n        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n    }\n    document.getElementById(tabName).style.display = \"block\"; // Ensure selected tab content is shown\n    evt.currentTarget.className += \" active\";\n }\n</script>\n```\n\n\n\n\n\n## Learning Outcomes\n\n\n<details>\n\n<summary>Characterize the properties of an $AR(p)$ stochastic process</summary>\n-   Define an $AR(p)$ stochastic process\n-   Express an $AR(p)$ process using the backward shift operator\n-   State an $AR(p)$ forecast (or prediction) function\n-   Identify stationarity of an $AR(p)$ process using the backward shift operator\n-   Determine the stationarity of an $AR(p)$ process using a characteristic equation\n\n</details>\n\n\n\n<details>\n\n<summary>Check model adequacy using diagnostic plots like correlograms of residuals</summary>\n-   Characterize a random walk's second order characteristics using a correlogram\n-   Define partial autocorrelations\n-   Explain how to use a partial correlogram to decide what model would be suitable to estimate an $AR(p)$ process\n-   Demonstrate the use of partial correlogram via simulation\n\n</details>\n\n\n\n\n\n\n## Preparation\n\n-   Read Section 4.5\n\n\n\n## Learning Journal Exchange (10 min)\n\n-   Review another student's journal\n\n-   What would you add to your learning journal after reading another student's?\n\n-   What would you recommend the other student add to their learning journal?\n\n-   Sign the Learning Journal review sheet for your peer\n\n\n\n\n## Class Activity: Definition of Autoregressive (AR) Models (10 min)\n\n<a id=\"ARdefinition\">We</a> now define an autoregressive (or AR) model.\n\n::: {.callout-note icon=false title=\"Definition of an Autoregressive (AR) Model\"}\n\nThe time series $\\{x_t\\}$ is an **autoregressive process of order $p$**, denoted as $AR(p)$, if\n$$\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n$$\n\nwhere $\\{w_t\\}$ is white noise and the $\\alpha_i$ are the model parameters with $\\alpha_p \\ne 0$.\n\n:::\n\nIn short, this means that the next observation of a time series depends linearly on the previous $p$ terms and a random white noise component. \n\nCheck your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\n-   Show that we can write Equation (4.15) as a polynomial of order $p$ in terms of the backward shift operator:\n$$\n   \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t = w_t\n$$\n\n:::\n\nWe have seen some special cases of this model already.\n\nCheck your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\n-   Give another name for an $AR(0)$ model.\n\n-   Show that the random walk is the special case $AR(1)$ with $\\alpha_1 = 1$. (<a href=\"https://byuistats.github.io/timeseries/chapter_4_lesson_1.html#randomwalk\">See Chapter 4, Lesson 1</a>.)\n\n-   Show that the exponential smoothing model is the special case where \n$$\\alpha_i = \\alpha(1-\\alpha)^i$$\n  for $i = 1, 2, \\ldots$ and $p \\rightarrow \\infty$. (<a href=\"https://byuistats.github.io/timeseries/chapter_3_lesson_2.html#ewma\">See Chapter 3, Lesson 2</a>.)\n\n:::\n\nWe now explore the autoregressive properties of this model.\n\nCheck your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\n-   Show that the $AR(p)$ model is a regression of $x_t$ on past terms from the same series. \nHint: write the $AR(p)$ model in more familiar terms, letting \n$$y_i = x_t, ~~ x_1 = x_{t-1}, ~~ x_2 = x_{t-2}, ~~ \\ldots, ~~ x_p = x_{t-p}, ~~ \\text{and} ~~ \\epsilon_i = w_t$$\n\n-   Explain why the prediction at time $t$ is given by\n$$\n    \\hat x_t = \\hat \\alpha_1 x_{t-1} + \\hat \\alpha_2 x_{t-2} + \\cdots + \\hat \\alpha_{p-1} x_{t-(p-1)} + \\hat \\alpha_p x_{t-p}\n$$\n  \n-   Explain why the model parameters (the $\\alpha$'s) can be estimated by minimizing the sum of the squared error terms: \n$$\\sum_{t=1}^n \\left( \\hat w_t \\right)^2 = \\sum_{t=1}^n \\left( x_t - \\hat x_t \\right)^2$$\n\n-   What is the reason this is called an autoregressive model?\n\n:::\n\n\n\n\n\n## Class Activity: Exploring $AR(1)$ Models (10 min)\n\n### Definition\n\nRecall that an $AR(p)$ model is of the form\n$$\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n$$\nSo, an $AR(1)$ model is expressed as\n$$\n  x_t = \\alpha x_{t-1} + w_t\n$$\nwhere $\\{w_t\\}$ is a white noise series with mean zero and variance $\\sigma^2$.\n\n### Second-Order Properties of an $AR(1)$ Model\n\nWe now explore the second-order properties of this model. \n\n::: {.callout-note icon=false title=\"Second-Order Properties of an $AR(1)$ Model\"}\n\nIf $\\{x_t\\}_{t=1}^n$ is an $AR(1)$ prcess, then its the first- and second-order properties are summarized below.\n\n$$\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n$$\n\n::: {.callout-tip title=\"Click here for a proof of the equation for $cov(x_t,x_{t+k})$\" collapse=true}\n\nWhy is $cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}$?\n\nIf $\\{x_t\\}$ is a stable $AR(1)$ process (which means that $|\\alpha|<1) can be written as:\n\n\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\n\nFrom this, we can deduce that the mean is \n\n$$\n  E(x_t) \n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n$$\n\nThe autocovariance is computed similarly as:\n\n\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k}) \n    &= cov \\left( \n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\nSee Equations (2.15) and (4.2).\n\n:::\n\n:::\n\n\n\n### Correlogram of an $AR(1)$ process\n\n::: {.callout-note icon=false title=\"Correlogram of an AR(1) Process\"}\n\nThe autocorrelation function for an AR(1) process is\n\n$$\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n$$\nwhere $|\\alpha| < 1$. \n\n:::\n\n\nCheck Your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\n-   Use the equation for the autocovariance function, $\\gamma_k$, to show that \n$$\n  \\rho_k = \\alpha^k\n$$\nfor $k \\ge 0$ when $|\\alpha|<1$.\n\n-   Use this to explain why the correlogram decays to zero more quickly when $\\alpha$ is small.\n\n:::\n\n### Small Group Activity: Simulation of an $AR(1)$ Process\n\n\n\n\n```{=html}\n <iframe id=\"AR1_app\" src=\"https://posit.byui.edu/content/be8ec2cd-6209-4b53-a5c6-be11464b86de\" style=\"border: none; width: 100%; height: 1100px\" frameborder=\"0\"></iframe>\n```\n\n\n\n\n\nCheck Your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\nIn each of the following cases, what do you observe in the correlogram? (If you expect to see significant results and you do not, try increasing the number of points.)\n\n-   $\\alpha = 1$\n-   $\\alpha = 0.5$\n-   $\\alpha = 0.1$\n-   $\\alpha = 0$   \n-   $\\alpha = -0.1$\n-   $\\alpha = -0.5$\n-   $\\alpha = -1$\n\n:::\n\n\n## Class Activity: Partial Autocorrelation (10 min)\n\n### Definition of Partial Autocorrelation\n\n::: {.callout-note icon=false title=\"Partial Autocorrleation\"}\n\nThe **partial autocorrelation** at lag $k$ is defined as the portion of the correlation that is not explained by shorter lags.\n\n:::\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\nCheck Your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\n-   What is the value of the partial autocorrelation function for an $AR(2)$ process for all lags greater than 2?\nZero \n\n:::\n\nOn page 81, the textbook states that in general, the partial autocorrelation at lag $k$ is the $k^{th}$ coefficient of a fitted $AR(k)$ model.\nThis implies that if the underlying process is $AR(p)$, then all the coefficients $\\alpha_k=0$ if $k>p$. So, an $AR(p)$ process will yield partial correlations that are zero after lag $p$. So, a correlogram of partial autocorrelations can be helpful to determine the order of an appropriate $AR$ process to model a time series.\n\n::: {.callout-note icon=false title=\"ACF and PACF of an $AR(p)$ Process\"}\n\nFor an $AR(p)$ process, we observe the following:\n\n<center>\n\n|      | AR(p)                  |\n|------|------------------------|\n| ACF  | Tails off              |\n| PACF | Cuts off after lag $p$ |\n\n</center>\n\nhttps://people.cs.pitt.edu/~milos/courses/cs3750/lectures/class16.pdf \n\n|      | AR(p)                  | MA(q)                  | ARMA(p,q)                | \n|------|------------------------|------------------------|--------------------------| \n| ACF  | Tails off              | Cuts off after lag $q$ | Tails off                | \n| PACF | Cuts off after lag $p$ | Tails off              | Tails off                | \n\n:::\n\n\n\n### Example: McDonald's Stock Price\n\nHere is a partial autocorrelation plot for the McDonald's stock price data:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Set symbol and date range\nsymbol <- \"MCD\"\ncompany <- \"McDonald's\"\n\n# Retrieve static file\nstock_df <- rio::import(\"https://byuistats.github.io/timeseries/data/stock_price_mcd.parquet\")\n\n# Transform data into tibble\nstock_ts <- stock_df %>%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %>%\n  select(dates, value) %>%\n  as_tibble() %>% \n  arrange(dates) |>\n  mutate(diff = value - lag(value)) |>\n  as_tsibble(index = dates, key = NULL) \n\npacf(stock_ts$value, plot=TRUE, lag.max = 25)\n```\n\n::: {.cell-output-display}\n![](chapter_4_lesson_3_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nThe only significant partial correlation is at lag $k=1$. This suggests that an $AR(1)$ process could be used to model the McDonald's stock prices.\n\n### Partial Autocorrelation Plots of Various $AR(p)$ Processes\n\nHere are some time plots, correlograms, and partial correlograms for $AR(p)$ processes with various values of $p$.\n\n### Shiny App\n\nnot working\n\n### google sheets test \n\n\n\n\n```{=html} \n <iframe id=\"googlesheets\" src=\"https://docs.google.com/spreadsheets/d/1K0yK3Ck08FBgPp8eDxw4aa7NhwDvuhTvb04U0ZjqB4A/edit?usp=sharing\" style=\"border: none; width: 100%; height: 1100px\" frameborder=\"0\"></iframe> \n``` \n\n\n\n\n\n\n\n\n## Class Activity: Stationary and Non-Stationary AR Processes (15 min)\n\n\n::: {.callout-note icon=false title=\"Definition of the Characteristic Equation\"}\n\nTreating the symbol $\\mathbf{B}$ formally as a number (either real or complex), the polynomial \n\n$$\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n$$\n\nis called the **characteristic polynomial** of an AR process. \n\nIf we set the characteristic polynomial to zero, we get the **characteristic equation**:\n\n$$\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n$$\n\n:::\n\nThe roots of the characteristic polynomial are the values of $\\mathbf{B}$ that make the polynomial equal to zero--i.e., the values of $\\mathbf{B}$ that make $\\theta_p(\\mathbf{B}) = 0$. These are also called the solutions of the characteristic equation.\nThe roots of the characteristic polynomial can be real or complex numbers.\n\nWe now explore an important result for AR processes that uses the absolute value of complex numbers.\n\n::: {.callout-note icon=false title=\"Identifying Stationary Processes\"}\n\nAn AR process will be **stationary** if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n:::\n\nFirst, we will find the roots of the characteristic polynomial (i.e. the solutions of the characteristic equation) and then we will determine if the absolute value of these solutions is greater than 1.\n\nWe can use the `polyroot` function to find the roots of polynomials in R.  For example, to find the roots of the polynomial $x^2-x-6$, we apply the command\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolyroot(c(-6,-1,1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  3+0i -2+0i\n```\n\n\n:::\n:::\n\n\n\n\nNote the order of the coefficients. They are given in increasing order of the power of $x$.\n\nOf course, we could simply factor the polynomial:\n$$\n  x^2-x-6 = (x-3)(x+2) \\overset{set}{=} 0\n$$\nwhich implies that\n$$\n  x = 3 ~~~ \\text{or} ~~~ x = -2\n$$\n\n::: {.callout-note icon=false title=\"Definition of the Absolute Value in the Complex Plane\"}\n\nLet $z = a+bi$ be any complex number. It can be represented by the point $(a,b)$ in the complex plane.  We define the absolute value of $z$ as the distance from the origin to the point:\n\n$$\n  |z| = \\sqrt{a^2 + b^2}\n$$\n\n:::\n\nPractice computing the absolute value of a complex number. $\\ $\n\n\nCheck Your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\nFind the absolute value of the following (complex) numbers:\n\n-   $-3$\n\n-   $4i$\n\n-   $-3+4i$\n\n-   $-\\dfrac{\\sqrt{3}}{4} + \\dfrac{1}{4} i$\n\n-   $\\dfrac{1}{\\sqrt{2}} - \\dfrac{1}{\\sqrt{2}} i$\n\n-   $5-12i$\n\n:::\n\nWe will now practice assessing whether an AR process is stationary using the characteristic equation. \n\nCheck Your Understanding \n\n::: {.callout-tip icon=false title=\"Check Your Understanding\"}\n\nFor each of the following AR processes, do the following:\n\n1.    Write the AR process in terms of the backward shift operator.\n2.    Solve the characteristic equation.\n3.    Determine if the AR process is stationary.\n\n-   $AR(1)$ process:\n$$\n    x_t = x_{t-1} + w_t\n$$\n$\\mathbf{B} = 1$ \nNot stationary \n\n-   $AR(1)$ process:\n$$\n    x_t = \\frac{1}{3} x_{t-1} + w_t\n$$\n$\\mathbf{B} = 3$ \nStationary \n\n-   $AR(2)$ process:\n$$\n    x_t = - \\frac{1}{4} x_{t-1} + \\frac{1}{8} x_{t-2} + w_t\n$$\n$$ -\\frac{1}{8} \\left(\\mathbf{B}^2 - 2 \\mathbf{B} - 8 \\right) x_t = w_t $$ \nB = -4, B = 2 \nStationary \n\n\n-   $AR(2)$ process:\n$$\n    x_t = - \\frac{2}{3} x_{t-1} + \\frac{1}{3} x_{t-2} + w_t\n$$\n$$ -\\frac{1}{3} \\left(\\mathbf{B}^2 - 2 \\mathbf{B} - 3 \\right) x_t = w_t $$ \nB = -1, B = 3 \nNon-Stationary \n\n\n-   $AR(2)$ process:\n$$\n    x_t = -x_{t-1} - 2 x_{t-2} + w_t\n$$\n$$ \\left(\\mathbf{B}^2 + 1/2 * \\mathbf{B} + 1/2 \\right) x_t = w_t $$ \nB = 1/4 +/- \\frac{sqrt{7}}{4} i \nNon-Stationary \n\n\n-   $AR(2)$ process:\n$$\n    x_t = \\frac{3}{2} x_{t-1} - x_{t-2} + w_t\n$$\n$$ \\left(\\mathbf{B}^2 - \\frac{3}{2} \\mathbf{B} + 1 \\right) x_t = w_t $$ \nB = \\frac{3}{4} +/- \\frac{sqrt{7}}{4} i  \n|B| = 1 \nNon-Stationary \n\n\n-   $AR(2)$ process:\n$$\n    x_t = 4 x_{t-2} + w_t\n$$\n$$ \\left(1 - 4\\mathbf{B}^2 \\right) x_t = w_t $$ \nB = \\pm \\frac{1}{2}  \n|B| = \\frac{1}{2} \nNon-stationary \n\n\n-   $AR(3)$ process:\n$$\n    x_t = \\frac{2}{3} x_{t-1} + \\frac{1}{4} x_{t-2} - \\frac{1}{6} x_{t-3} + w_t\n$$\nB = 2, -2, 3/2  \nStationary \n\n4.    Choose one stationary $AR(2)$ process and one non-stationary $AR(2)$ process. For each, do the following:\n\n-    Simulate at least 1000 sequential observations.\n-    Make a time plot of the simulated values.\n-    Make a correlogram of the simulated values.\n-    Plot the partial correlogram of the simulated values.\n\n    The following code chunk may be helpful--or you can use the simulation above.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Number of observations\nn_obs <- 1000\n\n# Generate sequence of dates\nstart_date <- my(paste(1, floor(year(now())-n_obs/365)))\ndate_seq <- seq(start_date,\n    start_date + days(n_obs - 1),\n    by = \"1 days\")\n\n# Simulate random component\nw <- rnorm(n_obs)\n\n# Set first few values of x\nx <- rep(0, n_obs)\nx[1] <- w[1]\nx[2] <- 0.6 * x[1] + w[2]\n\n# Set all remaining values of x\nfor (t in 3:n_obs) {\n  x[t] <- 0.6 * x[t-1] + 0.25 * x[t-2] + w[t]\n}\n\n# Create the tsibble\nsim_ts <- data.frame(dates = date_seq, x = x) |>\n  as_tsibble(index = dates) \n\n# Generate the plots\nsim_ts |> autoplot(.vars = x)\nacf(sim_ts$x, plot=TRUE, lag.max = 25)\npacf(sim_ts$x, plot=TRUE, lag.max = 25)\n```\n:::\n\n\n\n\n\n5.    What do you observe about the difference in the behavior of the stationary and non-stationary processes?\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Homework Preview (5 min)\n\n-   Review upcoming homework assignment\n-   Clarify questions\n\n\n::: {.callout-note icon=false}\n\n## Download Homework\n\n<a href=\"https://byuistats.github.io/timeseries/homework/homework_4_3.qmd\" download=\"homework_4_3.qmd\"> homework_4_3.qmd </a>\n\n:::\n\n\n\n\n\n\n\n\n\nCheck your Understanding \n\n\n-   Show that we can write Equation (4.15) as a polynomial of order $p$ in terms of the backward shift operator:\n$$\n   \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t = w_t\n$$\n\n**Solution:**\n  \n$$\n    x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t \n$$\n  After subtracting, we have:\n$$\n  \\begin{align*}\n    w_t \n      &= x_t - \\alpha_1 x_{t-1} - \\alpha_2 x_{t-2} - \\alpha_3 x_{t-3} - \\cdots - \\alpha_{p-1} x_{t-(p-1)} - \\alpha_p x_{t-p} \\\\\n      &= x_t - \\alpha_1 \\mathbf{B} x_{t} - \\alpha_2 \\mathbf{B}^2 x_{t} - \\alpha_3 \\mathbf{B}^3 x_{t} - \\cdots - \\alpha_{p-1} \\mathbf{B}^{p-1} x_{t} - \\alpha_p \\mathbf{B}^p x_{t} \\\\\n      &= \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2  - \\alpha_3 \\mathbf{B}^3 - \\cdots - \\alpha_{p-1} \\mathbf{B}^{p-1} - \\alpha_p \\mathbf{B}^p \\right) x_t\n  \\end{align*}\n$$\n\n<hr></hr> \n\n-   Give another name for an $AR(0)$ model.\n\n**Solution:**\n\nWhite noise\n\n<hr></hr> \n  \n-   Show that the random walk is the special case $AR(1)$ with $\\alpha_1 = 1$. (<a href=\"https://byuistats.github.io/timeseries/chapter_4_lesson_1.html#randomwalk\">See Chapter 4, Lesson 1</a>.)\n\n**Solution:**\n\n  If we let $\\alpha_1=1$ in an $AR(1)$ model, we get:\n$$\n  \\begin{align*}\n  x_t &= \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t \\\\\n    &= \\alpha_1 x_{t-1} + w_t \\\\\n    &= x_{t-1} + w_t \\\\\n  \\end{align*}\n$$\n  which is the definition of a random walk, as given in <a href=\"https://byuistats.github.io/timeseries/chapter_4_lesson_1.html#randomwalk\">Chapter 4, Lesson 1</a>.\n\n<hr></hr> \n\n-   Show that the exponential smoothing model is the special case where \n$$\\alpha_i = \\alpha(1-\\alpha)^i$$\n  for $i = 1, 2, \\ldots$ and $p \\rightarrow \\infty$. (<a href=\"https://byuistats.github.io/timeseries/chapter_3_lesson_2.html#ewma\">See Chapter 3, Lesson 2</a>.)\n\n**Solution:**\n\\begin{align*}\n    x_t &= \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t \\\\\n      &= \\alpha(1-\\alpha)^1 x_{t-1} + \\alpha(1-\\alpha)^2 x_{t-2} + \\alpha(1-\\alpha)^3 x_{t-3} + \\cdots + \\alpha(1-\\alpha)^{p-1} x_{t-(p-1)} + \\alpha(1-\\alpha)^p x_{t-p} + w_t \\\\\n\\end{align*}\n\n  \n  This is Equation (3.18) in <a href=\"https://byuistats.github.io/timeseries/chapter_3_lesson_2.html#ewma\">Chapter 3, Lesson 2</a>.\n\n<hr></hr> \n\n\n-   Show that the $AR(p)$ model is a regression of $x_t$ on previous terms in the series. (This is why it is called an \"autoregressive model.\") \nHint: write the $AR(p)$ model in more familiar terms, letting \n$$y_i = x_t, ~~ x_1 = x_{t-1}, ~~ x_2 = x_{t-2}, ~~ \\ldots, ~~ x_p = x_{t-p}, ~~ \\epsilon_i = w_t, ~~ \\text{and} ~~ \\beta_j = \\alpha_j$$\n\n**Solution:**\n$$\n\\begin{align*}\n    x_t &= \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t \\\\\n    y_i &= \\beta_1 x_{1i} ~+~ \\beta_2 x_{2i} ~+~ \\beta_3 x_{3i} ~+ \\cdots +~ \\beta_{p-1,i} x_{p-1,i} ~~~+~ \\beta_p x_{p,i} + \\epsilon_i \\\\\n\\end{align*}\n$$\n\n  This is a multiple linear regression equation with zero intercept.\n\n<hr></hr> \n\n-   Explain why the prediction at time $t$ is given by\n$$\n    \\hat x_t = \\hat \\alpha_1 x_{t-1} + \\hat \\alpha_2 x_{t-2} + \\cdots + \\hat \\alpha_{p-1} x_{t-(p-1)} + \\hat \\alpha_p x_{t-p}\n$$\n  \n**Solution:**\n\n  The prediction at time $t$ in a multiple regression setting would be:\n$$\n    \\hat y_i = \\hat \\beta_1 x_{1i} ~~~+~ \\hat \\beta_2 x_{2i} ~+~ \\hat \\beta_3 x_{3i} ~~+ \\cdots +~~ \\hat \\beta_{p-1} x_{p-1,i} ~~+~ \\hat \\beta_p x_{p,i}\n$$\n  Translated to the $AR(p)$ setting, this becomes:\n$$\n    \\hat x_t = \\hat \\alpha_1 x_{t-1} + \\hat \\alpha_3 x_{t-2} + \\hat \\alpha_3 x_{t-3} + \\cdots + \\hat \\alpha_{p-1} x_{t-(p-1)} + \\hat \\alpha_p x_{t-p}\n$$\n\n<hr></hr> \n\n-   Explain why the model parameters (the $\\alpha$'s) can be estimated by minimizing the sum of the squared error terms: \n$$\\sum_{t=1}^n \\left( \\hat w_t \\right)^2 = \\sum_{t=1}^n \\left( x_t - \\hat x_t \\right)^2$$\n\n**Solution:**\n\n  This is exactly how the multiple linear regression coefficients are estimated...minimizing the sum of the squared error terms.\n\n<hr></hr> \n\n-   What is the reason this is called an autoregressive model?\n\n**Solution:**\n\n  This is called an autoregressive model because we regress the current tern on the previous terms in the series.\n\n\n\n\n\n\n\nCheck Your Understanding Solutions\n\nFind the absolute value of the following (complex) numbers:\n\n-   $|-3| = \\sqrt{(-3)^2 + 0^2} = 3$\n\n-   $|4i| = \\sqrt{(0)^2 + (4)^2} = 4$\n\n-   $|-3+4i| = \\sqrt{(-3)^2 + (4)^2} = 5$\n\n-   $\\left| - \\dfrac{\\sqrt{3}}{4} + \\dfrac{1}{4} i \\right| = \\sqrt{\\left( \\dfrac{\\sqrt{-3}}{4} \\right)^2 + \\left( \\dfrac{1}{4} \\right)^2} = \\sqrt{\\dfrac{3}{16} + \\dfrac{1}{16}} = \\dfrac{1}{2}$\n\n-   $\\left| \\dfrac{1}{\\sqrt{2}} - \\dfrac{1}{\\sqrt{2}} i \\right| = \\sqrt{\\left( \\frac{1}{\\sqrt{2}} \\right)^2 + \\left( \\frac{-1}{\\sqrt{2}} \\right)^2} = 1$\n\n-   $|5-12i| = \\sqrt{(5)^2 + (-12)^2} = 13$\n\n\n\n\n\n\nCheck Your Understanding Solutions\n\nFor each of the following AR processes, do the following:\n\n1.    Write the AR process in terms of the backward shift operator.\n\n2.    Solve the characteristic equation.\n\n3.    Determine if the AR process is stationary.\n\n-   $AR(1)$ process:\n$$\n    x_t = x_{t-1} + w_t\n$$\n  \n$\\mathbf{B} = 1$ \nNot stationary \n\n**Solution:**\n\n\n\n<hr></hr>\n\n\n-   $AR(1)$ process:\n$$\n    x_t = \\frac{1}{3} x_{t-1} + w_t\n$$\n\n**Solution:**\n\n$$\\mathbf{B} = 3$$\nThis is a stationary AR process.\n\n<hr></hr>\n\n\n-   $AR(2)$ process:\n$$\n    x_t = - \\frac{1}{4} x_{t-1} + \\frac{1}{8} x_{t-2} + w_t\n$$\n\n**Solution:**\n\n$$ -\\frac{1}{8} \\left(\\mathbf{B}^2 - 2 \\mathbf{B} - 8 \\right) x_t = w_t $$\n$$B = -4, ~~~ B = 2$$\nThis is a stationary AR process.\n\n<hr></hr>\n\n\n-   $AR(2)$ process:\n$$\n    x_t = - \\frac{2}{3} x_{t-1} + \\frac{1}{3} x_{t-2} + w_t\n$$\n\n**Solution:**\n\n$$ -\\frac{1}{3} \\left(\\mathbf{B}^2 - 2 \\mathbf{B} - 3 \\right) x_t = w_t $$\n$$B = -1, ~~~ B = 3$$\nThis is a non-stationary AR process.\n\n<hr></hr>\n\n\n-   $AR(2)$ process:\n$$\n    x_t = -x_{t-1} - 2 x_{t-2} + w_t\n$$\n\n**Solution:**\n  \n$$ \\left(\\mathbf{B}^2 + 1/2 * \\mathbf{B} + 1/2 \\right) x_t = w_t $$\n$$ B = \\frac{1}{4} \\pm \\frac{\\sqrt{7}}{4} i $$\n$$ |B| = \\sqrt{\\frac{1}{16} + \\frac{7}{16} i } = \\frac{1}{\\sqrt{2}} \\le 1 $$\nThis is a non-stationary AR process.\n\n<hr></hr>\n\n\n-   $AR(2)$ process:\n$$\n    x_t = \\frac{3}{2} x_{t-1} - x_{t-2} + w_t\n$$\n\n**Solution:**\n\n$$ \\left(\\mathbf{B}^2 - \\frac{3}{2} \\mathbf{B} + 1 \\right) x_t = w_t $$\n$$ B = \\frac{3}{4} \\pm \\frac{\\sqrt{7}}{4} i $$\n$$|B| = 1$$\nThis is a non-stationary AR process.\n\n<hr></hr>\n\n\n-   $AR(2)$ process:\n$$\n    x_t = 4 x_{t-2} + w_t\n$$\n\n**Solution:**\n\n$$ \\left(1 - 4\\mathbf{B}^2 \\right) x_t = w_t $$\n$$B = \\pm \\frac{1}{2}$$\n$$|B| = \\frac{1}{2}$$\nThis is a non-stationary AR process.\n\n<hr></hr>\n\n\n-   $AR(3)$ process:\n$$\n    x_t = \\frac{2}{3} x_{t-1} + \\frac{1}{4} x_{t-2} - \\frac{1}{6} x_{t-3} + w_t\n$$\n\n**Solution:**\n\n$$ B = 2, ~ -2, ~ \\frac{3}{2} $$\nThis is a stationary AR process.\n\n\n\n\n\n",
    "supporting": [
      "chapter_4_lesson_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}