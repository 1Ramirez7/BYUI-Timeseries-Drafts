---
title: "Chapter 2: Lesson 3"
subtitle: "Exploration of Autocorrelation Concepts"
format: html
editor: source
sidebar: false
---

```{r}
#| include: false
source("../common_functions.R")
library(tidyverse)
```

```{=html}
<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
 }    
</script>
```

```{r}
#| include: false

get_data_for_cov_table <- function(offset = 1) {
  # set random number generator seed
  set.seed(92401)
  set.seed(16633)
  set.seed(2686)
  set.seed(997)
  
  # set parameters
  n <- 10
  rho <- 0.99
  mu <- 0
  sigma <- 3
  
  # build population correlation matrix
  tmp.r <- matrix(rho, n, n)
  tmp.r <- tmp.r^abs(row(tmp.r)-col(tmp.r))
  
  # simulate correlated normal random data
  x1 <- round(mvrnorm(1, rep(mu,n), sigma^2 * tmp.r),1) 
  
  # build a data frame
  df <- data.frame(t = 1:length(x1),
                   x = x1,
                   y = lead(x1, offset)) |>
    mutate(
      xx = x - mean(x),
      xx2 = xx^2,
      yy = y - mean(x),
      # yy2 = yy^2,
      xy = xx * yy
    ) |> 
    dplyr::select(t, x, y, xx, xx2, yy, xy)
  
  return(df)
}

make_cov_table_df <- function(df, offset=1, decimals_1st_order = 5, decimals_2nd_order = 5) {
  # Color vector
  oi_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#F5C710", "#CC79A7", "#999999")
  
  df_summary <- df |> 
    summarize(
      x = sum(x),
      y = sum(y, na.rm = TRUE),
      xx = sum(xx),
      yy = sum(yy, na.rm = TRUE),
      xx2 = sum(xx2),
      # yy2 = sum(yy2, na.rm = TRUE),
      xy = sum(xy, na.rm = TRUE)
    ) |> 
    # round_df(3) |> 
    mutate(t = paste0("sum")) |> 
    mutate(
      t = paste0("sum"),
      x = round_to_places(x, 1),
      y = round_to_places(y, 1),
      xx = round_to_places(xx, decimals_1st_order),
      xx2 = round_to_places(xx2, decimals_2nd_order),
      yy = round_to_places(yy, decimals_1st_order),
      # yy2 = round_to_places(yy2, decimals_2nd_order),
      xy = round_to_places(xy, decimals_2nd_order)
    ) |> 
    dplyr::select(t, x, y, xx, xx2, yy, xy)
  
  out <- df |>
    mutate(
      t = as.character(t),
      x = round_to_places(x, 1),
      y = round_to_places(y, 1),
      xx = round_to_places(xx, decimals_1st_order),
      xx2 = round_to_places(xx2, decimals_2nd_order),
      yy = round_to_places(yy, decimals_1st_order),
      # yy2 = round_to_places(yy2, decimals_2nd_order),
      xy = round_to_places(xy, decimals_2nd_order)
    ) |> 
    mutate(
      x = cell_spec(x, 
                    color = case_when(
                      is.na(x) ~ "#999999",
                      TRUE ~ oi_colors[( row_number() + 0 ) %% 9 + 1]
                    )
      ),
      y = cell_spec(y, 
                    color = case_when(
                      is.na(y) ~ "#999999",
                      TRUE ~ oi_colors[( row_number() + offset ) %% 9 + 1]
                    )
      )
    ) |> 
    mutate(
      # x = ifelse(row_number() > nrow(.) - offset, paste0("[",x,"]"), x),
      y = ifelse(row_number() > nrow(.) - offset, NA, y),
    ) |> 
    replace(., is.na(.), "â€”") |>
    bind_rows(df_summary) |> 
    rename(
      "x_t" = x,
      "x_{t+k}" = y,
      # paste0("x_{t+", offset, "}") = y,
      "x_t-mean(x)" = xx, 
      "(x_t-mean(x))^2" = xx2, 
      "x_{t+k}-mean(x)" = yy,
      # "(x_{t+k}-mean(x))^2" = yy2, 
      "(x-mean(x))(x_{t+k}-mean(x))" = xy
    )
  
  return(out)
}

# Compute summary values
compute_summaries <- function(df, digits = 4) {
  df |> 
    summarize(
      mean_x = mean(x),
      mean_y = mean(y, na.rm = TRUE),
      ss_x = sum(xx2),
      # ss_y = sum(yy2, na.rm = TRUE),
      ss_xy = sum(xy, na.rm = TRUE),
      c_0 = sum(xx2) / nrow(.),
      c_k = sum(xy, na.rm = TRUE) / nrow(.),
      r_k = c_k / c_0
    ) |> 
    round_df(digits)
}

```


## Learning Outcomes

<details>
<summary>Explain the theoretical implications of autocorrelation for the estimation of time series statistics</summary>
    -   Explain how positive autocorrelation leads to underestimation of variance in short time series
    -   Explain how negative autocorrelation can improve efficiency of sample mean estimate
</details>
<details>
<summary>Explain why there is not one correct model to describe a time series</summary>
    -   Explain why there can be several suitable models for a given time series
</details>
<details>
<summary>Compute the covariance of sums of random variables</summary>
    -   Define the covariance of sums of random variables.
</details>
<details>
<summary>Interpret correlograms to identify significant lags, correlations, trends, and seasonality</summary>
    -   Create a correlogram
    -   Interpret a correlogram
    -   Define a sampling distribution
    -   State the sampling distribution of rk
    -   Explain the concept of a confidence interval
    -   Conduct a single hypothesis test using a correlogram
    -   Describe the problems associated with multiple hypothesis testing in a correlogram
    -   Differentiate statistical and practical significance
    -   Diagnose non-stationarity using a correlogram
</details>



## Preparation

-   Read Sections 2.2.5 and 2.3-2.5 (No new reading assignment)


<!-- ## Learning Journal Exchange (10 mins) -->

<!-- -   Review another student's journal -->
<!-- -   What would you add to your learning journal after reading your partner's? -->
<!-- -   What would you recommend your partner add to their learning journal? -->
<!-- -   Sign the Learning Journal review sheet for your peer -->


## Introduction (5 mins)


### Correlograms

-   The table below gives the sample autocorrelation function, acf, for the data set from the previous lesson. Use these values to sketch a correlogram in your Learning Journal. The figure below can help you begin.

```{r}
#| echo: false

x <- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )
df <- data.frame(x = x)

z <- acf(x, plot=FALSE, type = "correlation") 
# acf(x, plot=TRUE, type = "correlation") ## Solution

# z

z$acf |> 
  data.frame() |> 
  round_df(3) |> 
  rename("acf" = "z.acf") |> 
  mutate(
    k = row_number() - 1,
    acf = as.character(acf)
  ) |> 
  pivot_wider(names_from = k, values_from = acf) |> 
  # mutate("4" = "_____") |> 
  display_table()

ggplot(data = df, aes(x = seq_along(x), y = acf(x, plot = FALSE)$acf)) +
  # geom_col() +
  ylim(-1, 1) +
  scale_x_continuous(breaks = 0:9) + 
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 1)) + 
  geom_segment(aes(x = 0, y = 0, xend = 9, yend = 0)) + ## Hack
  geom_hline(yintercept = 0, linetype = "solid", linewidth=1, color = "black") +
  geom_hline(yintercept = (2.6/4.2), linetype = "dashed", linewidth=1, color = "#0072B2") +  # Texbooks says these lines should be at (-0.1 +/- 2/sqrt(10)). Used +/-(2.6/4.2), based on measurements made visually with a ruler from the figure generated by R.
  geom_hline(yintercept = (-2.6/4.2), linetype = "dashed", linewidth=1, color = "#0072B2") +
  labs(x = "Lag", y = "ACF") +
  # theme_bw()   
  # theme(panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank())
  theme_bw() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank()
  )
```

<!-- -   Finish a partially-created correlogram with the r_k values computed -->

-   Are any of the autocorrelations statistically significant? If so, which one(s)?

## Application: Chocolate Search Trends

Recall the Google Trends data for the term "chocolate" from the last lesson.
The cleaned data are available in the file <a href="../data/chocolate.csv" download>chocolate.csv</a>.

### Import the chocolate search data and convert to tsibble format

Use the code below to import the data and convert it into a time series (tsibble) object.

```{r}

# load packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load("tsibble", "fable",
               "feasts", "tsibbledata",
               "fable.prophet", "tidyverse",
               "patchwork", "rio")

# read in the data from a csv and make the tsibble
# change the line below to include your file path
chocolate_month <- rio::import("../data/chocolate.csv")
start_date <- lubridate::ymd("2004-01-01")
date_seq <- seq(start_date,
                start_date + months(nrow(chocolate_month)-1),
                by = "1 months")
chocolate_tibble <- tibble(
  dates = date_seq,
  year = lubridate::year(date_seq),
  month = lubridate::month(date_seq),
  value = pull(chocolate_month, chocolate)
)
chocolate_month_ts <- chocolate_tibble |>
  mutate(index = tsibble::yearmonth(dates)) |>
  as_tsibble(index = index)

# generate the plot
choc_plot <- autoplot(chocolate_month_ts, .vars = value) +
  labs(
    x = "Month",
    y = "Searches",
    title = "Relative Number of Google Searches for 'Chocolate'"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

choc_plot
```

### Explore R commands summarizing time series data

-   What does each of the following R commands give us?
    -   class(chocolate_month_ts)
    -   head(chocolate_month_ts, 1)
    -   tail(chocolate_month_ts, 1)
    -   guess_frequency(chocolate_month_ts$index)

### Aggregate to the annual level

To help visualize what is happening with a time series, we often aggregate the data in the time series to the annual level. This can make it easier to spot a trend.
The first plot is the time series plot of the raw data, and the second plot is a time series plot of the annual means.

```{r}
chocolate_annual_ts <- summarise(index_by(chocolate_month_ts, year), value = mean(value))

# monthly plot
mp <- autoplot(chocolate_month_ts, .vars = value) +
  labs(
    x = "Month",
    y = "Searches",
    title = "Relative Number of Google Searches for 'Chocolate'"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

# yearly plot
yp <- autoplot(chocolate_annual_ts, .vars = value) +
  labs(
    x = "Year",
    y = "Searches",
    title = "Mean Annual Google Searches for 'Chocolate'"
  ) +
  scale_x_continuous(breaks = seq(2004, max(chocolate_month_ts$year), by = 2)) +
  theme(plot.title = element_text(hjust = 0.5))

# display both simultaneously
mp / yp
```

-   What do you observe in the trend?
-   What do you suspect is causing this trend?

### Boxplot of Seasonal Values

To better visualize the effect of seasonal variation, we can make box plots by month.

```{r}
pbox <- ggplot(chocolate_month_ts, aes(x = factor(month), y = value)) +
    geom_boxplot() +
  labs(
    x = "Month Number",
    y = "Searches",
    title = "Boxplots of Google Searches for 'Chocolate' by Month"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

pbox
```
-   What do you observe? 
-   Which months tend to have the most searches?  Which months tend to have the fewest seraches?
    -   Can you provide an explanation for this?


## Additive Decomposition 

The additive model for a time series, $\{x_t\}$, is:

$$
  x_t = m_t + s_t + z_t
$$

where $x_t$ is the observed series, $m_t$ is the trend, $s_t$ is the seasonal effect, $z_t$ is random variation, and $t = 1, 2, 3, \ldots, n$. We consider $z_t$ as a sequence of correlated random variables with mean 0.

The residual is the part of the time series that is not explained by the trend or the seasonal effect. This is an estimate of $
For an additive model, the residual is computed as:
$$
  Residual = Value - Trend - Seasonal
$$

The code below produces a plot showing the original data in the time series, the trend, the seasonal effect, and the residual (random component). 

```{r}
#| warning: false

choc_decompose <- chocolate_month_ts |>
    model(feasts::classical_decomposition(value,
        type = "add"))  |>
    components()

autoplot(choc_decompose)
```


::: callout-caution
## TYSON: We need to read in the chocolate search trends data, decompose it, then compute the autocorrelations on residuals. What do we need to do to make this work?
:::

-   Is there a trend in the data? If so, speculate about the cause of this trend.
-   Is there evidence of seasonal variation? If so, propose an explanation for the seasonal variation.
-   What do the spikes in the correlogram tell us about this time series?
-   Is there evidence of autocorrelations in the data after removing the trend and seasonal variation?
-   There are two exceptionally large spikes in the observed data. Can you identify a cause for these spikes?


## Recap (5 mins)

-   Review objectives and key points
-   Review the definitions of autocovariance, autocorrelation
-   Discuss autocorrelation in the time series context



<a href="javascript:showhide('Solutions')"
style="font-size:.8em;">Correlograms</a>

::: {#Solutions style="display:none;"}

Solutions to correlogram activity

```{r}
x <- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )
acf(x, plot=FALSE, type = "correlation")
acf(x, plot=TRUE, type = "correlation")
```

:::
