---
title: "Autoregressive Models: Part 2 (MAYBE RECOMBINE)"
subtitle: "Chapter 4: Lesson 4"
format: html
editor: source
sidebar: false
---

```{r}
#| include: false
source("common_functions.R")
```

```{=html}
<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
 }    
</script>
```


## Learning Outcomes

{{< include outcomes/chapter_4_lesson_4_outcomes.qmd >}}




## Preparation

-   Read Sections 4.5.3-4.5.7



## Learning Journal Exchange (10 mins)

-   Review another student's journal

-   What would you add to your learning journal after reading another student's?

-   What would you recommend the other student add to their learning journal?

-   Sign the Learning Journal review sheet for your peer


## Class Activity: Exploring $AR(1)$ Models (20 min)

### Recall the Definition

Recall that an $AR(p)$ model is of the form
$$
  x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \alpha_3 x_{t-3} + \cdots + \alpha_{p-1} x_{t-(p-1)} + \alpha_p x_{t-p} + w_t
$$
So, an $AR(1)$ model is expressed as
$$
  x_t = \alpha x_{t-1} + w_t
$$
where $\{w_t\}$ is a white noise series with mean zero and variance $\sigma^2$. 

### Second-Order Properties of an $AR(1)$ Model

We now explore the second-order properties of this model. 

::: {.callout-note icon=false title="Second-Order Properties of an $AR(1)$ Model"}

If $\{x_t\}_{t=1}^n$ is an $AR(1)$ prcess, then its the first- and second-order properties are summarized below.

$$
\begin{align*}
  \mu_x &= 0 \\  
  \gamma_k = cov(x_t, x_{t+k}) &= \frac{\alpha^k \sigma^2}{1-\alpha^2}
\end{align*}
$$

::: {.callout-tip title="Click here for a proof of the equation for $cov(x_t,x_{t+k})$" collapse=true}

Why is $cov(x_t, x_{t+k}) = \dfrac{\alpha^k \sigma^2}{1-\alpha^2}$?

If $\{x_t\}$ is a stable $AR(1)$ process (which means that $|\alpha|<1) can be written as:

\begin{align*}
  (1-\alpha \mathbf{B}) x_t &= w_t \\
  \implies x_t &= (1-\alpha \mathbf{B})^{-1} w_t \\
    &= w_t + \alpha w_{t-1} + \alpha^2 w_{t-2} + \alpha^3 w_{t-3} + \cdots \\
    &= \sum\limits_{i=0}^\infty \alpha^i w_{t-i}
\end{align*}

From this, we can deduce that the mean is 

$$
  E(x_t) 
    = E\left( \sum\limits_{i=0}^\infty \alpha^i w_{t-i} \right)
    = \sum\limits_{i=0}^\infty \alpha^i E\left( w_{t-i} \right)
    = 0
$$

The autocovariance is computed similarly as:

\begin{align*}
  \gamma_k = cov(x_t, x_{t+k}) 
    &= cov \left( 
      \sum\limits_{i=0}^\infty \alpha^i w_{t-i}, \\
      \sum\limits_{j=0}^\infty \alpha^j w_{t+k-j} \right) \\
    &= \sum\limits_{j=k+i} \alpha^i \alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\
    &= \alpha^k \sigma^2 \sum\limits_{i=0}^\infty \alpha^{2i} \\
    &= \frac{\alpha^k \sigma^2}{1-\alpha^2}
\end{align*}

See Equations (2.15) and (4.2).

:::

:::



### Correlogram of an $AR(1)$ process

::: {.callout-note icon=false title="Correlogram of an AR(1) Process"}

The autocorrelation function for an AR(1) process is

$$
  \rho_k = \alpha^k ~~~~~~ (k \ge 0)
$$
where $|\alpha| \le 1$. 

:::


<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Use the equation for the autocovariance function, $\gamma_k$, to show that 
$$
  \rho_k = \alpha^k
$$
for $k \ge 0$ when $|\alpha|<1$.

-   Use this to explain why the correlogram decays to zero more quickly when $\alpha$ is small.

:::

### Small Group Activity: Simulation of an $AR(1)$ Process

```{=html}
 <iframe id="AR1_app" src="https://posit.byui.edu/content/be8ec2cd-6209-4b53-a5c6-be11464b86de" style="border: none; width: 100%; height: 1100px" frameborder="0"></iframe>
```


<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

In each of the following cases, what do you observe in the correlogram? (If you expect to see significant results and you do not, try increasing the number of points.)

-   $\alpha = 1$
-   $\alpha = 0.5$
-   $\alpha = 0.1$
-   $\alpha = 0$   
-   $\alpha = -0.1$
-   $\alpha = -0.5$
-   $\alpha = -1$

:::


## Class Activity: Partial Autocorrelation (10 min)

### Definition of Partial Autocorrelation

::: {.callout-note icon=false title="Partial Autocorrleation"}

The **partial autocorrelation** at lag $k$ is defined as the portion of the correlation that is not explained by shorter lags.

:::

For example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.


<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   What is the value of the partial autocorrelation function for an $AR(2)$ process for all lags greater than 2?
<!-- Zero -->

:::

On page 81, the textbook states that in general, the partial autocorrelation at lag $k$ is the $k^{th}$ coefficient of a fitted $AR(k)$ model.
This implies that if the underlying process is $AR(p)$, then all the coefficients $\alpha_k=0$ if $k>p$. So, an $AR(p)$ process will yeild partial correlations that are zero after lag $p$. So, a correlogram of partial autocorrelations can be helpful to determine the order of an appropriate $AR$ process to model a time series.

### Example: McDonald's Stock Price

Here is a partial autocorrelation plot for the McDonald's stock price data:


```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Set symbol and date range
symbol <- "MCD"
company <- "McDonald's"

# Retrieve static file
stock_df <- rio::import("data/stock_price_mcd.parquet")

# Transform data into tibble
stock_ts <- stock_df %>%
  mutate(
    dates = date, 
    value = adjusted
  ) %>%
  select(dates, value) %>%
  as_tibble() %>% 
  arrange(dates) |>
  mutate(diff = value - lag(value)) |>
  as_tsibble(index = dates, key = NULL) 

pacf(stock_ts$value, plot=TRUE, lag.max = 25)
```

The only significant partial correlation is at lag $k=1$. This suggests that an $AR(1)$ process could be used to model the McDonald's stock prices.

### Partial Autocorrelation Plots of Various $AR(p)$ Processes

Here are some time plots, correlograms, and partial correlograms for $AR(p)$ processes with various values of $p$.

::: panel-tabset

#### $AR(1)$

::: {.callout-tip appearance="minimal"}

```{r}
#| echo: false

alpha_1 <- 0.7
```

$$
  x_t = `r alpha_1` x_{t-1} + w_t
$$

```{r}
#| echo: false
#| fig-asp: 0.5

# AR(1) process
n_days <- 500
x <- w <- rnorm(n_days)
start_date <- my(paste(1, floor(year(now())-n_days/365)))
date_seq <- seq(start_date,
    start_date + days(n_days - 1),
    by = "1 days")

for (t in 2:n_days) {
  x[t] <- alpha_1 * x[t-1] + w[t]
}

sim_ts <- data.frame(dates = date_seq, x=x) |>
  as_tsibble(index = dates)

sim_ts |>
  autoplot(.vars = x)

acf(sim_ts$x, main = "ACF of Simulated AR(1) Process")

pacf(sim_ts$x, main = "Partial ACF of Simulated AR(1) Process")
```

:::

#### $AR(2)$

::: {.callout-tip appearance="minimal"}

```{r}
#| echo: false

alpha_1 <- 0.5
alpha_2 <- 0.5
```

$$
  x_t = `r alpha_1` x_{t-1} + `r alpha_2` x_{t-2} + w_t
$$

```{r}
#| echo: false
#| fig-asp: 0.5

# AR(2) process
x <- w <- rnorm(n_days)
start_date <- my(paste(1, floor(year(now())-n_days/365)))
date_seq <- seq(start_date,
    start_date + days(n_days - 1),
    by = "1 days")

x[2] <- alpha_1 * x[1] + w[2]
for (t in 3:n_days) {
  x[t] <- alpha_1 * x[t-1] + alpha_2 * x[t-2] + w[t]
}

sim_ts <- data.frame(dates = date_seq, x=x) |>
  as_tsibble(index = dates)

sim_ts |>
  autoplot(.vars = x)

acf(sim_ts$x, main = "ACF of Simulated AR(2) Process")

pacf(sim_ts$x, main = "Partial ACF of Simulated AR(2) Process")
```

:::

#### $AR(3)$

::: {.callout-tip appearance="minimal"}

```{r}
#| echo: false

alpha_1 <- 0.25
alpha_2 <- 0.2
alpha_3 <- 0.15
```

$$
  x_t = `r alpha_1` x_{t-1} + `r alpha_2` x_{t-2} + `r alpha_3` x_{t-3} + w_t
$$

```{r}
#| echo: false
#| fig-asp: 0.5

# AR(3) process
x <- w <- rnorm(n_days)
start_date <- my(paste(1, floor(year(now())-n_days/365)))
date_seq <- seq(start_date,
    start_date + days(n_days - 1),
    by = "1 days")

x[2] <- alpha_1 * x[1] + w[2]
x[3] <- alpha_1 * x[2] + alpha_2 * x[1] + w[3]
for (t in 4:n_days) {
  x[t] <- alpha_1 * x[t-1] + alpha_2 * x[t-2] + alpha_3 * x[t-3] + w[t]
}

sim_ts <- data.frame(dates = date_seq, x=x) |>
  as_tsibble(index = dates)

sim_ts |>
  autoplot(.vars = x)

acf(sim_ts$x, main = "ACF of Simulated AR(3) Process")

pacf(sim_ts$x, main = "Partial ACF of Simulated AR(3) Process")
```

:::

:::
<!-- End of the panel-tabset -->



<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Question1
-   Question2

:::

# Finish previous section



<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Stuff goes here
-   Stuff goes here

:::











<!-- Check your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Question1
-   Question2

:::





## Homework Preview (5 min)

-   Review upcoming homework assignment
-   Clarify questions


::: {.callout-note icon=false}

## Download Homework

<a href="https://byuistats.github.io/timeseries/homework/homework_4_4.qmd" download="homework_4_4.qmd"> homework_4_4.qmd </a>

:::


<a href="javascript:showhide('Solutions1')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions1 style="display:none;"}
    

:::




<a href="javascript:showhide('Solutions')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions2 style="display:none;"}
    

:::




<a href="javascript:showhide('Solutions3')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions3 style="display:none;"}
    

:::



