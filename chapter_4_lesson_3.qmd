---
title: "Autoregressive Models: Part 1"
subtitle: "Chapter 4: Lesson 3"
format: html
editor: source
sidebar: false
---

```{r}
#| include: false
source("common_functions.R")
```

```{=html}
<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
 }    
</script>
```


## Learning Outcomes

{{< include outcomes/chapter_4_lesson_3_outcomes.qmd >}}




## Preparation

-   Read Section 4.5.1 and 4.5.2



## Learning Journal Exchange (10 mins)

-   Review another student's journal

-   What would you add to your learning journal after reading another student's?

-   What would you recommend the other student add to their learning journal?

-   Sign the Learning Journal review sheet for your peer




## Class Activity: Autoregressive Model Definition (20 min)

::: {.callout-note icon=false title="Definition of an Autoregressive Model"}

The time series $\{x_t\}$ is an **autoregressive process of order $p$**, denoted as $AR(p)$, if
$$
  x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \alpha_3 x_{t-3} + \cdots + \alpha_{p-1} x_{t-(p-1)} + \alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)
$$

where $\{w_t\}$ is white noise and the $\alpha_i$ are the model parameters with $\alpha_p \ne 0$.

:::

<!-- Check your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Show that we can write Equation (4.15) as a polynomial of order $p$ in terms of the backward shift operator:
  $$
    \left( 1 - \alpha_1 \mathbf{B} - \alpha_2 \mathbf{B}^2 - \cdots - \alpha_p \mathbf{B}^p \right) x_t = w_t
  $$
-   Show that the random walk is the special case $AR(1)$ with $\alpha_1 = 1$. (<a href="https://byuistats.github.io/timeseries/chapter_4_lesson_1.html#randomwalk">See Chapter 4, Lesson 1</a>.)

-   Show that the exponential smoothing model is the special case where 
  $$\alpha_i = \alpha(1-\alpha)^i$$
  for $i = 1, 2, \ldots$ and $p \rightarrow \infty$. (<a href="https://byuistats.github.io/timeseries/chapter_3_lesson_2.html#ewma">See Chapter 3, Lesson 2</a>.)

-   Show that the $AR(p)$ model is a regression of $x_t$ on past terms from the same series. (This is why it is called an "autoregressive model.") 
Hint: write the $AR(p)$ model in more familiar terms, letting 
  $$y_i = x_t, ~~ x_1 = x_{t-1}, ~~ x_2 = x_{t-2}, ~~ \ldots, x_p = x_{t-p}, ~~ \text{and} ~~ \epsilon_i = w_t$$

-   Explain why the prediction at time $t$ is given by
  $$
    \hat x_t = \hat \alpha_1 x_{t-1} + \hat \alpha_2 x_{t-2} + \cdots + \hat \alpha_{p-1} x_{t-(p-1)} + \hat \alpha_p x_{t-p}
  $$
  
-   Explain why the model parameters (the $\alpha$'s) can be estimated by minimizing the sum of the squared error terms: 
  $$\sum_{t=1}^n \left( \hat w_t \right)^2 = \sum_{t=1}^n \left( x_t - \hat x_t \right)^2$$

:::







## Small Group Activity: SectionTitle (xxx min)
## Class Activity: SectionTitle (xxx min)


<!-- Check your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Question1
-   Question2

:::





## Homework Preview (5 min)

-   Review upcoming homework assignment
-   Clarify questions


::: {.callout-note icon=false}

## Download Homework

<a href="https://byuistats.github.io/timeseries/homework/homework_4_3.qmd" download="homework_4_3.qmd"> homework_4_3.qmd </a>

:::


<a href="javascript:showhide('Solutions1')"
style="font-size:.8em;">Check Your Understanding: Autoregressive Model Definition</a>
  
::: {#Solutions1 style="display:none;"}

<!-- Check your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Show that we can write Equation (4.15) as a polynomial of order $p$ in terms of the backward shift operator:
  $$
    \left( 1 - \alpha_1 \mathbf{B} - \alpha_2 \mathbf{B}^2 - \cdots - \alpha_p \mathbf{B}^p \right) x_t = w_t
  $$

**Solution:**
  
  $$
    x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \alpha_3 x_{t-3} + \cdots + \alpha_{p-1} x_{t-(p-1)} + \alpha_p x_{t-p} + w_t \\
  $$
  After subtracting, we have:
  \begin{align*}
    w_t 
      &= x_t - \alpha_1 x_{t-1} - \alpha_2 x_{t-2} - \alpha_3 x_{t-3} - \cdots - \alpha_{p-1} x_{t-(p-1)} - \alpha_p x_{t-p} \\
      &= x_t - \alpha_1 \mathbf{B} x_{t} - \alpha_2 \mathbf{B}^2 x_{t} - \alpha_3 \mathbf{B}^3 x_{t} - \cdots - \alpha_{p-1} \mathbf{B}^{p-1} x_{t} - \alpha_p \mathbf{B}^p x_{t} \\
      &= \left( 1 - \alpha_1 \mathbf{B} - \alpha_2 \mathbf{B}^2  - \alpha_3 \mathbf{B}^3 - \cdots - \alpha_{p-1} \mathbf{B}^{p-1} - \alpha_p \mathbf{B}^p \right) x_t
  \end{align*}


-   Show that the random walk is the special case $AR(1)$ with $\alpha_1 = 1$. (<a href="https://byuistats.github.io/timeseries/chapter_4_lesson_1.html#randomwalk">See Chapter 4, Lesson 1</a>.)

**Solution:**

  If we let $\alpha_1=1$ in an $AR(1)$ model, we get:
  
  \begin{align*}
  x_t &= \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \alpha_3 x_{t-3} + \cdots + \alpha_{p-1} x_{t-(p-1)} + \alpha_p x_{t-p} + w_t \\
    &= \alpha_1 x_{t-1} + w_t \\
    &= x_{t-1} + w_t \\
  \end{align*}
  
  which is the definition of a random walk, as given in <a href="https://byuistats.github.io/timeseries/chapter_4_lesson_1.html#randomwalk">Chapter 4, Lesson 1</a>.


-   Show that the exponential smoothing model is the special case where 
  $$\alpha_i = \alpha(1-\alpha)^i$$
  for $i = 1, 2, \ldots$ and $p \rightarrow \infty$. (<a href="https://byuistats.github.io/timeseries/chapter_3_lesson_2.html#ewma">See Chapter 3, Lesson 2</a>.)

**Solution:**

  \begin{align*}
    x_t &= \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \alpha_3 x_{t-3} + \cdots + \alpha_{p-1} x_{t-(p-1)} + \alpha_p x_{t-p} + w_t \\
      &= \alpha(1-\alpha)^1 x_{t-1} + \alpha(1-\alpha)^2 x_{t-2} + \alpha(1-\alpha)^3 x_{t-3} + \cdots + \alpha(1-\alpha)^{p-1} x_{t-(p-1)} + \alpha(1-\alpha)^p x_{t-p} + w_t \\
  \end{align*}

  This is Equation (3.18) in <a href="https://byuistats.github.io/timeseries/chapter_3_lesson_2.html#ewma">Chapter 3, Lesson 2</a>.


-   Show that the $AR(p)$ model is a regression of $x_t$ on past terms from the same series. (This is why it is called an "autoregressive model.") 
Hint: write the $AR(p)$ model in more familiar terms, letting 
  $$y_i = x_t, ~~ x_1 = x_{t-1}, ~~ x_2 = x_{t-2}, ~~ \ldots, x_p = x_{t-p}, ~~ \text{and} ~~ \epsilon_i = w_t$$

**Solution:**




-   Explain why the prediction at time $t$ is given by
  $$
    \hat x_t = \hat \alpha_1 x_{t-1} + \hat \alpha_2 x_{t-2} + \cdots + \hat \alpha_{p-1} x_{t-(p-1)} + \hat \alpha_p x_{t-p}
  $$
  
**Solution:**




-   Explain why the model parameters (the $\alpha$'s) can be estimated by minimizing the sum of the squared error terms: 
  $$\sum_{t=1}^n \left( \hat w_t \right)^2 = \sum_{t=1}^n \left( x_t - \hat x_t \right)^2$$

**Solution:**




:::

:::




<a href="javascript:showhide('Solutions')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions2 style="display:none;"}
    

:::




<a href="javascript:showhide('Solutions3')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions3 style="display:none;"}
    

:::



